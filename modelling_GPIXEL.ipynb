{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31cf957f-1a95-4c98-8032-3e4babbe104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Standard library\n",
    "# ================================\n",
    "import os\n",
    "import random\n",
    "\n",
    "# ================================\n",
    "# Data manipulation\n",
    "# ================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ================================\n",
    "# Visualization\n",
    "# ================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ================================\n",
    "# Signal processing\n",
    "# ================================\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# ================================\n",
    "# Preprocessing\n",
    "# ================================\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ================================\n",
    "# Model selection / CV\n",
    "# ================================\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    StratifiedKFold\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# ML models — Regression\n",
    "# ================================\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# ================================\n",
    "# ML models — Classification\n",
    "# ================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ================================\n",
    "# Metrics\n",
    "# ================================\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# PyTorch (for NN models)\n",
    "# ================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b314545-292f-4db5-bfb0-f621a17c09de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_name</th>\n",
       "      <th>graph_type</th>\n",
       "      <th>pixel_0</th>\n",
       "      <th>pixel_1</th>\n",
       "      <th>pixel_2</th>\n",
       "      <th>pixel_3</th>\n",
       "      <th>pixel_4</th>\n",
       "      <th>pixel_5</th>\n",
       "      <th>pixel_6</th>\n",
       "      <th>pixel_7</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_246</th>\n",
       "      <th>pixel_247</th>\n",
       "      <th>pixel_248</th>\n",
       "      <th>pixel_249</th>\n",
       "      <th>pixel_250</th>\n",
       "      <th>pixel_251</th>\n",
       "      <th>pixel_252</th>\n",
       "      <th>pixel_253</th>\n",
       "      <th>pixel_254</th>\n",
       "      <th>pixel_255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>barabasi_n1000_m10_1</td>\n",
       "      <td>barabasi</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.043137</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>barabasi_n1000_m10_10</td>\n",
       "      <td>barabasi</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.043137</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>barabasi_n1000_m10_11</td>\n",
       "      <td>barabasi</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>barabasi_n1000_m10_12</td>\n",
       "      <td>barabasi</td>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.074510</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>barabasi_n1000_m10_13</td>\n",
       "      <td>barabasi</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.074510</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.011765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>barabasi_n1000_m10_14</td>\n",
       "      <td>barabasi</td>\n",
       "      <td>0.160784</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>barabasi_n1000_m10_15</td>\n",
       "      <td>barabasi</td>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.007843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           instance_name graph_type   pixel_0   pixel_1   pixel_2   pixel_3  \\\n",
       "0   barabasi_n1000_m10_1   barabasi  0.176471  0.043137  0.062745  0.082353   \n",
       "1  barabasi_n1000_m10_10   barabasi  0.117647  0.054902  0.058824  0.062745   \n",
       "2  barabasi_n1000_m10_11   barabasi  0.184314  0.047059  0.058824  0.062745   \n",
       "3  barabasi_n1000_m10_12   barabasi  0.168627  0.066667  0.062745  0.074510   \n",
       "4  barabasi_n1000_m10_13   barabasi  0.200000  0.050980  0.074510  0.082353   \n",
       "5  barabasi_n1000_m10_14   barabasi  0.160784  0.050980  0.070588  0.062745   \n",
       "6  barabasi_n1000_m10_15   barabasi  0.168627  0.047059  0.066667  0.070588   \n",
       "\n",
       "    pixel_4   pixel_5   pixel_6   pixel_7  ...  pixel_246  pixel_247  \\\n",
       "0  0.062745  0.058824  0.054902  0.047059  ...   0.011765   0.011765   \n",
       "1  0.050980  0.043137  0.047059  0.039216  ...   0.015686   0.011765   \n",
       "2  0.070588  0.050980  0.058824  0.054902  ...   0.011765   0.015686   \n",
       "3  0.050980  0.062745  0.050980  0.054902  ...   0.015686   0.011765   \n",
       "4  0.050980  0.058824  0.054902  0.058824  ...   0.015686   0.011765   \n",
       "5  0.066667  0.058824  0.050980  0.050980  ...   0.011765   0.015686   \n",
       "6  0.058824  0.062745  0.054902  0.050980  ...   0.011765   0.011765   \n",
       "\n",
       "   pixel_248  pixel_249  pixel_250  pixel_251  pixel_252  pixel_253  \\\n",
       "0   0.007843   0.011765   0.011765   0.007843   0.007843   0.011765   \n",
       "1   0.011765   0.011765   0.007843   0.007843   0.007843   0.011765   \n",
       "2   0.011765   0.011765   0.015686   0.011765   0.007843   0.007843   \n",
       "3   0.011765   0.011765   0.011765   0.011765   0.011765   0.011765   \n",
       "4   0.011765   0.011765   0.011765   0.007843   0.007843   0.007843   \n",
       "5   0.011765   0.011765   0.007843   0.011765   0.011765   0.007843   \n",
       "6   0.011765   0.007843   0.007843   0.011765   0.011765   0.007843   \n",
       "\n",
       "   pixel_254  pixel_255  \n",
       "0   0.007843   0.007843  \n",
       "1   0.011765   0.007843  \n",
       "2   0.007843   0.007843  \n",
       "3   0.007843   0.007843  \n",
       "4   0.007843   0.011765  \n",
       "5   0.007843   0.007843  \n",
       "6   0.007843   0.007843  \n",
       "\n",
       "[7 rows x 258 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT_DIR=\"instances_images\"\n",
    "CSV_PATH=os.path.join(OUT_DIR, \"graphs_images_flat_16.csv\")\n",
    "df=pd.read_csv(CSV_PATH)\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "696894aa-a9ad-49df-8c35-849c39e5a323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_cols=[c for c in df.columns if c.startswith(\"pixel_\")]\n",
    "len(pixel_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43c6c95c-2bba-4cf1-b28a-f9f8fb792b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "X = df[pixel_cols].values        #pixel_cols\n",
    "y = le.fit_transform(df[\"graph_type\"].values)      #etiquetas: barabasi / erdos / watt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d685bde4-ee3d-4017-ad45-839a018da5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12600dc2-5aaf-4945-85fa-15fd00d98785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41acc6d0-1631-4344-80bf-386af08932ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varianza explicada por cada componente: [0.69702856 0.1717936  0.04817497 0.01165558 0.00806597 0.00442965\n",
      " 0.00322616 0.00197148 0.00187405 0.00159513]\n",
      "Varianza acumulada: [0.69702856 0.86882216 0.91699713 0.92865271 0.93671868 0.94114833\n",
      " 0.94437449 0.94634597 0.94822002 0.94981515]\n"
     ]
    }
   ],
   "source": [
    "#% de varianza acumulada por los componentes de PCA\n",
    "print(\"Varianza explicada por cada componente:\", pca.explained_variance_ratio_)\n",
    "print(\"Varianza acumulada:\", pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc77e8ed-0d73-419f-a065-41ba5a4e2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fpca = X_pca[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0f80b178-bc71-4cff-9323-fb7140fa9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_fpca, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fba06ffa-7266-448d-a6e8-9d1316887921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Mejores hiperparámetros LogReg: {'C': 100, 'class_weight': 'balanced', 'penalty': 'l1'}\n",
      "               modelo  accuracy_train  precision_train  recall_train  \\\n",
      "0  LogisticRegression        0.847222         0.847345      0.847222   \n",
      "\n",
      "   f1_train  accuracy_test  precision_test  recall_test   f1_test  \n",
      "0  0.847196       0.861111        0.861196     0.861111  0.861096  \n"
     ]
    }
   ],
   "source": [
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    solver='liblinear',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_grid=param_grid_lr,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',   # podrías usar 'f1_weighted' si te interesa más\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros LogReg:\", grid_search_lr.best_params_)\n",
    "\n",
    "best_lr = grid_search_lr.best_estimator_\n",
    "\n",
    "def calcular_metricas(modelo, X, y, average='weighted'):\n",
    "    y_pred = modelo.predict(X)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred, average=average, zero_division=0),\n",
    "        'recall': recall_score(y, y_pred, average=average, zero_division=0),\n",
    "        'f1': f1_score(y, y_pred, average=average, zero_division=0)\n",
    "    }\n",
    "\n",
    "metricas_train_lr = calcular_metricas(best_lr, X_train, y_train)\n",
    "metricas_test_lr = calcular_metricas(best_lr, X_test, y_test)\n",
    "\n",
    "resultados_logreg = pd.DataFrame({\n",
    "    'modelo': ['LogisticRegression'],\n",
    "    'accuracy_train': [metricas_train_lr['accuracy']],\n",
    "    'precision_train': [metricas_train_lr['precision']],\n",
    "    'recall_train': [metricas_train_lr['recall']],\n",
    "    'f1_train': [metricas_train_lr['f1']],\n",
    "    'accuracy_test': [metricas_test_lr['accuracy']],\n",
    "    'precision_test': [metricas_test_lr['precision']],\n",
    "    'recall_test': [metricas_test_lr['recall']],\n",
    "    'f1_test': [metricas_test_lr['f1']]\n",
    "})\n",
    "\n",
    "print(resultados_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "42c6939f-2540-49c2-b0f4-4bff22308e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones en entrenamiento y test\n",
    "y_pred_train = best_lr.predict(X_train)\n",
    "y_pred_test = best_lr.predict(X_test)\n",
    "\n",
    "# Etiquetas de clase\n",
    "class_labels = ['BA','ER','WS']\n",
    "\n",
    "# ============================\n",
    "# Matriz de confusión - Train\n",
    "# ============================\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train,\n",
    "                                    display_labels=class_labels)\n",
    "disp_train.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Train\")\n",
    "plt.savefig(\"best_lr_train_confusion.png\", dpi=300, bbox_inches=\"tight\")  # Guardar\n",
    "plt.close()\n",
    "\n",
    "# ============================\n",
    "# Matriz de confusión - Test\n",
    "# ============================\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test,\n",
    "                                   display_labels=class_labels)\n",
    "disp_test.plot(cmap=\"Oranges\", values_format=\"d\")\n",
    "plt.title(\"Test\")\n",
    "plt.savefig(\"best_lr_test_confusion.png\", dpi=300, bbox_inches=\"tight\")  # Guardar\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "439d09a7-9520-450c-9afa-9b42fd40a5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Mejores hiperparámetros SVC: {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "  modelo  accuracy_train  precision_train  recall_train  f1_train  \\\n",
      "0    SVC        0.943576         0.943668      0.943576  0.943572   \n",
      "\n",
      "   accuracy_test  precision_test  recall_test   f1_test  \n",
      "0       0.940972         0.94124     0.940972  0.940958  \n"
     ]
    }
   ],
   "source": [
    "param_grid_svc = {\n",
    "    'C': [0.01,0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']  # relevante para rbf y poly\n",
    "}\n",
    "\n",
    "svc = SVC(random_state=42)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search_svc = GridSearchCV(\n",
    "    estimator=svc,\n",
    "    param_grid=param_grid_svc,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',   \n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros SVC:\", grid_search_svc.best_params_)\n",
    "\n",
    "best_svc = grid_search_svc.best_estimator_\n",
    "\n",
    "def calcular_metricas(modelo, X, y, average='weighted'):\n",
    "    y_pred = modelo.predict(X)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred, average=average, zero_division=0),\n",
    "        'recall': recall_score(y, y_pred, average=average, zero_division=0),\n",
    "        'f1': f1_score(y, y_pred, average=average, zero_division=0)\n",
    "    }\n",
    "\n",
    "metricas_train_svc = calcular_metricas(best_svc, X_train, y_train)\n",
    "metricas_test_svc = calcular_metricas(best_svc, X_test, y_test)\n",
    "\n",
    "resultados_svc = pd.DataFrame({\n",
    "    'modelo': ['SVC'],\n",
    "    'accuracy_train': [metricas_train_svc['accuracy']],\n",
    "    'precision_train': [metricas_train_svc['precision']],\n",
    "    'recall_train': [metricas_train_svc['recall']],\n",
    "    'f1_train': [metricas_train_svc['f1']],\n",
    "    'accuracy_test': [metricas_test_svc['accuracy']],\n",
    "    'precision_test': [metricas_test_svc['precision']],\n",
    "    'recall_test': [metricas_test_svc['recall']],\n",
    "    'f1_test': [metricas_test_svc['f1']]\n",
    "})\n",
    "\n",
    "print(resultados_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "edb73954-e447-40d2-ae56-2ddf81662a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones en entrenamiento y test\n",
    "y_pred_train = best_svc.predict(X_train)\n",
    "y_pred_test = best_svc.predict(X_test)\n",
    "\n",
    "# Etiquetas de clase\n",
    "class_labels = ['BA','ER','WS']\n",
    "\n",
    "# ============================\n",
    "# Matriz de confusión - Train\n",
    "# ============================\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train,\n",
    "                                    display_labels=class_labels)\n",
    "disp_train.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Train\")\n",
    "plt.savefig(\"best_svc_train_confusion.png\", dpi=300, bbox_inches=\"tight\")  # Guardar\n",
    "plt.close()\n",
    "\n",
    "# ============================\n",
    "# Matriz de confusión - Test\n",
    "# ============================\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test,\n",
    "                                   display_labels=class_labels)\n",
    "disp_test.plot(cmap=\"Oranges\", values_format=\"d\")\n",
    "plt.title(\"Test\")\n",
    "plt.savefig(\"best_svc_test_confusion.png\", dpi=300, bbox_inches=\"tight\")  # Guardar\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8d1168cf-d7f3-4b5e-b2b0-0639883dc789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Mejores hiperparámetros RF: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "         modelo  accuracy_train  precision_train  recall_train  f1_train  \\\n",
      "0  RandomForest             1.0              1.0           1.0       1.0   \n",
      "\n",
      "   accuracy_test  precision_test  recall_test   f1_test  \n",
      "0       0.989583        0.989618     0.989583  0.989583  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [10,50,100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros RF:\", grid_search_rf.best_params_)\n",
    "\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "def calcular_metricas(modelo, X, y, average='weighted'):\n",
    "    y_pred = modelo.predict(X)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred, average=average, zero_division=0),\n",
    "        'recall': recall_score(y, y_pred, average=average, zero_division=0),\n",
    "        'f1': f1_score(y, y_pred, average=average, zero_division=0)\n",
    "    }\n",
    "\n",
    "\n",
    "metricas_train = calcular_metricas(best_rf, X_train, y_train)\n",
    "metricas_test = calcular_metricas(best_rf, X_test, y_test)\n",
    "\n",
    "resultados_rf = pd.DataFrame({\n",
    "    'modelo': ['RandomForest'],\n",
    "    'accuracy_train': [metricas_train['accuracy']],\n",
    "    'precision_train': [metricas_train['precision']],\n",
    "    'recall_train': [metricas_train['recall']],\n",
    "    'f1_train': [metricas_train['f1']],\n",
    "    'accuracy_test': [metricas_test['accuracy']],\n",
    "    'precision_test': [metricas_test['precision']],\n",
    "    'recall_test': [metricas_test['recall']],\n",
    "    'f1_test': [metricas_test['f1']]\n",
    "})\n",
    "\n",
    "print(resultados_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7d914aae-9797-4f20-b8e5-bd2b8ed6ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones en entrenamiento y test\n",
    "y_pred_train = best_rf.predict(X_train)\n",
    "y_pred_test = best_rf.predict(X_test)\n",
    "\n",
    "# Etiquetas de clase\n",
    "class_labels = ['BA','ER','WS']\n",
    "\n",
    "# ============================\n",
    "# Matriz de confusión - Train\n",
    "# ============================\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train,\n",
    "                                    display_labels=class_labels)\n",
    "disp_train.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Train\")\n",
    "plt.savefig(\"best_rf_train_confusion.png\", dpi=300, bbox_inches=\"tight\")  # Guardar\n",
    "plt.close()\n",
    "\n",
    "# ============================\n",
    "# Matriz de confusión - Test\n",
    "# ============================\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test,\n",
    "                                   display_labels=class_labels)\n",
    "disp_test.plot(cmap=\"Oranges\", values_format=\"d\")\n",
    "plt.title(\"Test\")\n",
    "plt.savefig(\"best_rf_test_confusion.png\", dpi=300, bbox_inches=\"tight\")  # Guardar\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a966ad1a-dabf-4bab-aaca-be02adb6d1dd",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d2232e31-c774-4c38-b700-660f317a754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 1. Preparar datos\n",
    "# =========================================\n",
    "learning_rate = 0.01\n",
    "n_epochs = 2000\n",
    "hidden_size = 32\n",
    "\n",
    "# Codificar etiquetas (3 clases)\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df[\"graph_type\"].values)\n",
    "\n",
    "# X con 256 píxeles\n",
    "X = df.drop(['instance_name', 'graph_type', 'label'], axis=1)\n",
    "\n",
    "# y como la columna 'label'\n",
    "y = df['label']   # valores 0,1,2\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c38ece7d-eb25-4f88-943b-48c409b3e0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_cls(\n",
      "  (h1): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (out): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (act): ReLU()\n",
      ")\n",
      "Number of parameters: 8323\n",
      "Epoch [5/2000] - Loss: 1.0986\n",
      "Epoch [10/2000] - Loss: 1.0985\n",
      "Epoch [15/2000] - Loss: 1.0984\n",
      "Epoch [20/2000] - Loss: 1.0984\n",
      "Epoch [25/2000] - Loss: 1.0983\n",
      "Epoch [30/2000] - Loss: 1.0983\n",
      "Epoch [35/2000] - Loss: 1.0982\n",
      "Epoch [40/2000] - Loss: 1.0982\n",
      "Epoch [45/2000] - Loss: 1.0982\n",
      "Epoch [50/2000] - Loss: 1.0981\n",
      "Epoch [55/2000] - Loss: 1.0981\n",
      "Epoch [60/2000] - Loss: 1.0981\n",
      "Epoch [65/2000] - Loss: 1.0980\n",
      "Epoch [70/2000] - Loss: 1.0980\n",
      "Epoch [75/2000] - Loss: 1.0980\n",
      "Epoch [80/2000] - Loss: 1.0980\n",
      "Epoch [85/2000] - Loss: 1.0979\n",
      "Epoch [90/2000] - Loss: 1.0979\n",
      "Epoch [95/2000] - Loss: 1.0979\n",
      "Epoch [100/2000] - Loss: 1.0979\n",
      "Epoch [105/2000] - Loss: 1.0978\n",
      "Epoch [110/2000] - Loss: 1.0978\n",
      "Epoch [115/2000] - Loss: 1.0978\n",
      "Epoch [120/2000] - Loss: 1.0978\n",
      "Epoch [125/2000] - Loss: 1.0977\n",
      "Epoch [130/2000] - Loss: 1.0977\n",
      "Epoch [135/2000] - Loss: 1.0977\n",
      "Epoch [140/2000] - Loss: 1.0977\n",
      "Epoch [145/2000] - Loss: 1.0976\n",
      "Epoch [150/2000] - Loss: 1.0976\n",
      "Epoch [155/2000] - Loss: 1.0976\n",
      "Epoch [160/2000] - Loss: 1.0976\n",
      "Epoch [165/2000] - Loss: 1.0976\n",
      "Epoch [170/2000] - Loss: 1.0975\n",
      "Epoch [175/2000] - Loss: 1.0975\n",
      "Epoch [180/2000] - Loss: 1.0975\n",
      "Epoch [185/2000] - Loss: 1.0975\n",
      "Epoch [190/2000] - Loss: 1.0975\n",
      "Epoch [195/2000] - Loss: 1.0974\n",
      "Epoch [200/2000] - Loss: 1.0974\n",
      "Epoch [205/2000] - Loss: 1.0974\n",
      "Epoch [210/2000] - Loss: 1.0974\n",
      "Epoch [215/2000] - Loss: 1.0973\n",
      "Epoch [220/2000] - Loss: 1.0973\n",
      "Epoch [225/2000] - Loss: 1.0973\n",
      "Epoch [230/2000] - Loss: 1.0973\n",
      "Epoch [235/2000] - Loss: 1.0973\n",
      "Epoch [240/2000] - Loss: 1.0973\n",
      "Epoch [245/2000] - Loss: 1.0972\n",
      "Epoch [250/2000] - Loss: 1.0972\n",
      "Epoch [255/2000] - Loss: 1.0972\n",
      "Epoch [260/2000] - Loss: 1.0972\n",
      "Epoch [265/2000] - Loss: 1.0972\n",
      "Epoch [270/2000] - Loss: 1.0972\n",
      "Epoch [275/2000] - Loss: 1.0972\n",
      "Epoch [280/2000] - Loss: 1.0972\n",
      "Epoch [285/2000] - Loss: 1.0971\n",
      "Epoch [290/2000] - Loss: 1.0971\n",
      "Epoch [295/2000] - Loss: 1.0971\n",
      "Epoch [300/2000] - Loss: 1.0971\n",
      "Epoch [305/2000] - Loss: 1.0971\n",
      "Epoch [310/2000] - Loss: 1.0971\n",
      "Epoch [315/2000] - Loss: 1.0971\n",
      "Epoch [320/2000] - Loss: 1.0970\n",
      "Epoch [325/2000] - Loss: 1.0970\n",
      "Epoch [330/2000] - Loss: 1.0970\n",
      "Epoch [335/2000] - Loss: 1.0970\n",
      "Epoch [340/2000] - Loss: 1.0970\n",
      "Epoch [345/2000] - Loss: 1.0970\n",
      "Epoch [350/2000] - Loss: 1.0970\n",
      "Epoch [355/2000] - Loss: 1.0969\n",
      "Epoch [360/2000] - Loss: 1.0969\n",
      "Epoch [365/2000] - Loss: 1.0969\n",
      "Epoch [370/2000] - Loss: 1.0969\n",
      "Epoch [375/2000] - Loss: 1.0969\n",
      "Epoch [380/2000] - Loss: 1.0969\n",
      "Epoch [385/2000] - Loss: 1.0969\n",
      "Epoch [390/2000] - Loss: 1.0968\n",
      "Epoch [395/2000] - Loss: 1.0968\n",
      "Epoch [400/2000] - Loss: 1.0968\n",
      "Epoch [405/2000] - Loss: 1.0968\n",
      "Epoch [410/2000] - Loss: 1.0968\n",
      "Epoch [415/2000] - Loss: 1.0968\n",
      "Epoch [420/2000] - Loss: 1.0968\n",
      "Epoch [425/2000] - Loss: 1.0967\n",
      "Epoch [430/2000] - Loss: 1.0967\n",
      "Epoch [435/2000] - Loss: 1.0967\n",
      "Epoch [440/2000] - Loss: 1.0967\n",
      "Epoch [445/2000] - Loss: 1.0967\n",
      "Epoch [450/2000] - Loss: 1.0967\n",
      "Epoch [455/2000] - Loss: 1.0967\n",
      "Epoch [460/2000] - Loss: 1.0966\n",
      "Epoch [465/2000] - Loss: 1.0966\n",
      "Epoch [470/2000] - Loss: 1.0966\n",
      "Epoch [475/2000] - Loss: 1.0966\n",
      "Epoch [480/2000] - Loss: 1.0966\n",
      "Epoch [485/2000] - Loss: 1.0966\n",
      "Epoch [490/2000] - Loss: 1.0965\n",
      "Epoch [495/2000] - Loss: 1.0965\n",
      "Epoch [500/2000] - Loss: 1.0965\n",
      "Epoch [505/2000] - Loss: 1.0965\n",
      "Epoch [510/2000] - Loss: 1.0965\n",
      "Epoch [515/2000] - Loss: 1.0965\n",
      "Epoch [520/2000] - Loss: 1.0964\n",
      "Epoch [525/2000] - Loss: 1.0964\n",
      "Epoch [530/2000] - Loss: 1.0964\n",
      "Epoch [535/2000] - Loss: 1.0964\n",
      "Epoch [540/2000] - Loss: 1.0964\n",
      "Epoch [545/2000] - Loss: 1.0963\n",
      "Epoch [550/2000] - Loss: 1.0963\n",
      "Epoch [555/2000] - Loss: 1.0963\n",
      "Epoch [560/2000] - Loss: 1.0963\n",
      "Epoch [565/2000] - Loss: 1.0963\n",
      "Epoch [570/2000] - Loss: 1.0963\n",
      "Epoch [575/2000] - Loss: 1.0963\n",
      "Epoch [580/2000] - Loss: 1.0962\n",
      "Epoch [585/2000] - Loss: 1.0962\n",
      "Epoch [590/2000] - Loss: 1.0962\n",
      "Epoch [595/2000] - Loss: 1.0962\n",
      "Epoch [600/2000] - Loss: 1.0962\n",
      "Epoch [605/2000] - Loss: 1.0961\n",
      "Epoch [610/2000] - Loss: 1.0961\n",
      "Epoch [615/2000] - Loss: 1.0961\n",
      "Epoch [620/2000] - Loss: 1.0961\n",
      "Epoch [625/2000] - Loss: 1.0961\n",
      "Epoch [630/2000] - Loss: 1.0961\n",
      "Epoch [635/2000] - Loss: 1.0961\n",
      "Epoch [640/2000] - Loss: 1.0960\n",
      "Epoch [645/2000] - Loss: 1.0960\n",
      "Epoch [650/2000] - Loss: 1.0960\n",
      "Epoch [655/2000] - Loss: 1.0960\n",
      "Epoch [660/2000] - Loss: 1.0960\n",
      "Epoch [665/2000] - Loss: 1.0960\n",
      "Epoch [670/2000] - Loss: 1.0959\n",
      "Epoch [675/2000] - Loss: 1.0959\n",
      "Epoch [680/2000] - Loss: 1.0959\n",
      "Epoch [685/2000] - Loss: 1.0959\n",
      "Epoch [690/2000] - Loss: 1.0959\n",
      "Epoch [695/2000] - Loss: 1.0959\n",
      "Epoch [700/2000] - Loss: 1.0959\n",
      "Epoch [705/2000] - Loss: 1.0958\n",
      "Epoch [710/2000] - Loss: 1.0958\n",
      "Epoch [715/2000] - Loss: 1.0958\n",
      "Epoch [720/2000] - Loss: 1.0958\n",
      "Epoch [725/2000] - Loss: 1.0958\n",
      "Epoch [730/2000] - Loss: 1.0958\n",
      "Epoch [735/2000] - Loss: 1.0957\n",
      "Epoch [740/2000] - Loss: 1.0957\n",
      "Epoch [745/2000] - Loss: 1.0957\n",
      "Epoch [750/2000] - Loss: 1.0957\n",
      "Epoch [755/2000] - Loss: 1.0957\n",
      "Epoch [760/2000] - Loss: 1.0957\n",
      "Epoch [765/2000] - Loss: 1.0957\n",
      "Epoch [770/2000] - Loss: 1.0956\n",
      "Epoch [775/2000] - Loss: 1.0956\n",
      "Epoch [780/2000] - Loss: 1.0956\n",
      "Epoch [785/2000] - Loss: 1.0956\n",
      "Epoch [790/2000] - Loss: 1.0956\n",
      "Epoch [795/2000] - Loss: 1.0956\n",
      "Epoch [800/2000] - Loss: 1.0955\n",
      "Epoch [805/2000] - Loss: 1.0955\n",
      "Epoch [810/2000] - Loss: 1.0955\n",
      "Epoch [815/2000] - Loss: 1.0955\n",
      "Epoch [820/2000] - Loss: 1.0955\n",
      "Epoch [825/2000] - Loss: 1.0955\n",
      "Epoch [830/2000] - Loss: 1.0954\n",
      "Epoch [835/2000] - Loss: 1.0954\n",
      "Epoch [840/2000] - Loss: 1.0954\n",
      "Epoch [845/2000] - Loss: 1.0954\n",
      "Epoch [850/2000] - Loss: 1.0954\n",
      "Epoch [855/2000] - Loss: 1.0954\n",
      "Epoch [860/2000] - Loss: 1.0953\n",
      "Epoch [865/2000] - Loss: 1.0953\n",
      "Epoch [870/2000] - Loss: 1.0953\n",
      "Epoch [875/2000] - Loss: 1.0953\n",
      "Epoch [880/2000] - Loss: 1.0953\n",
      "Epoch [885/2000] - Loss: 1.0953\n",
      "Epoch [890/2000] - Loss: 1.0952\n",
      "Epoch [895/2000] - Loss: 1.0952\n",
      "Epoch [900/2000] - Loss: 1.0952\n",
      "Epoch [905/2000] - Loss: 1.0952\n",
      "Epoch [910/2000] - Loss: 1.0952\n",
      "Epoch [915/2000] - Loss: 1.0952\n",
      "Epoch [920/2000] - Loss: 1.0951\n",
      "Epoch [925/2000] - Loss: 1.0951\n",
      "Epoch [930/2000] - Loss: 1.0951\n",
      "Epoch [935/2000] - Loss: 1.0951\n",
      "Epoch [940/2000] - Loss: 1.0951\n",
      "Epoch [945/2000] - Loss: 1.0951\n",
      "Epoch [950/2000] - Loss: 1.0950\n",
      "Epoch [955/2000] - Loss: 1.0950\n",
      "Epoch [960/2000] - Loss: 1.0950\n",
      "Epoch [965/2000] - Loss: 1.0950\n",
      "Epoch [970/2000] - Loss: 1.0950\n",
      "Epoch [975/2000] - Loss: 1.0950\n",
      "Epoch [980/2000] - Loss: 1.0949\n",
      "Epoch [985/2000] - Loss: 1.0949\n",
      "Epoch [990/2000] - Loss: 1.0949\n",
      "Epoch [995/2000] - Loss: 1.0949\n",
      "Epoch [1000/2000] - Loss: 1.0949\n",
      "Epoch [1005/2000] - Loss: 1.0949\n",
      "Epoch [1010/2000] - Loss: 1.0948\n",
      "Epoch [1015/2000] - Loss: 1.0948\n",
      "Epoch [1020/2000] - Loss: 1.0948\n",
      "Epoch [1025/2000] - Loss: 1.0948\n",
      "Epoch [1030/2000] - Loss: 1.0948\n",
      "Epoch [1035/2000] - Loss: 1.0947\n",
      "Epoch [1040/2000] - Loss: 1.0947\n",
      "Epoch [1045/2000] - Loss: 1.0947\n",
      "Epoch [1050/2000] - Loss: 1.0947\n",
      "Epoch [1055/2000] - Loss: 1.0947\n",
      "Epoch [1060/2000] - Loss: 1.0947\n",
      "Epoch [1065/2000] - Loss: 1.0946\n",
      "Epoch [1070/2000] - Loss: 1.0946\n",
      "Epoch [1075/2000] - Loss: 1.0946\n",
      "Epoch [1080/2000] - Loss: 1.0946\n",
      "Epoch [1085/2000] - Loss: 1.0946\n",
      "Epoch [1090/2000] - Loss: 1.0945\n",
      "Epoch [1095/2000] - Loss: 1.0945\n",
      "Epoch [1100/2000] - Loss: 1.0945\n",
      "Epoch [1105/2000] - Loss: 1.0945\n",
      "Epoch [1110/2000] - Loss: 1.0945\n",
      "Epoch [1115/2000] - Loss: 1.0945\n",
      "Epoch [1120/2000] - Loss: 1.0944\n",
      "Epoch [1125/2000] - Loss: 1.0944\n",
      "Epoch [1130/2000] - Loss: 1.0944\n",
      "Epoch [1135/2000] - Loss: 1.0944\n",
      "Epoch [1140/2000] - Loss: 1.0944\n",
      "Epoch [1145/2000] - Loss: 1.0943\n",
      "Epoch [1150/2000] - Loss: 1.0943\n",
      "Epoch [1155/2000] - Loss: 1.0943\n",
      "Epoch [1160/2000] - Loss: 1.0943\n",
      "Epoch [1165/2000] - Loss: 1.0943\n",
      "Epoch [1170/2000] - Loss: 1.0943\n",
      "Epoch [1175/2000] - Loss: 1.0942\n",
      "Epoch [1180/2000] - Loss: 1.0942\n",
      "Epoch [1185/2000] - Loss: 1.0942\n",
      "Epoch [1190/2000] - Loss: 1.0942\n",
      "Epoch [1195/2000] - Loss: 1.0942\n",
      "Epoch [1200/2000] - Loss: 1.0941\n",
      "Epoch [1205/2000] - Loss: 1.0941\n",
      "Epoch [1210/2000] - Loss: 1.0941\n",
      "Epoch [1215/2000] - Loss: 1.0941\n",
      "Epoch [1220/2000] - Loss: 1.0941\n",
      "Epoch [1225/2000] - Loss: 1.0940\n",
      "Epoch [1230/2000] - Loss: 1.0940\n",
      "Epoch [1235/2000] - Loss: 1.0940\n",
      "Epoch [1240/2000] - Loss: 1.0940\n",
      "Epoch [1245/2000] - Loss: 1.0940\n",
      "Epoch [1250/2000] - Loss: 1.0939\n",
      "Epoch [1255/2000] - Loss: 1.0939\n",
      "Epoch [1260/2000] - Loss: 1.0939\n",
      "Epoch [1265/2000] - Loss: 1.0939\n",
      "Epoch [1270/2000] - Loss: 1.0939\n",
      "Epoch [1275/2000] - Loss: 1.0938\n",
      "Epoch [1280/2000] - Loss: 1.0938\n",
      "Epoch [1285/2000] - Loss: 1.0938\n",
      "Epoch [1290/2000] - Loss: 1.0938\n",
      "Epoch [1295/2000] - Loss: 1.0938\n",
      "Epoch [1300/2000] - Loss: 1.0937\n",
      "Epoch [1305/2000] - Loss: 1.0937\n",
      "Epoch [1310/2000] - Loss: 1.0937\n",
      "Epoch [1315/2000] - Loss: 1.0937\n",
      "Epoch [1320/2000] - Loss: 1.0937\n",
      "Epoch [1325/2000] - Loss: 1.0936\n",
      "Epoch [1330/2000] - Loss: 1.0936\n",
      "Epoch [1335/2000] - Loss: 1.0936\n",
      "Epoch [1340/2000] - Loss: 1.0936\n",
      "Epoch [1345/2000] - Loss: 1.0936\n",
      "Epoch [1350/2000] - Loss: 1.0935\n",
      "Epoch [1355/2000] - Loss: 1.0935\n",
      "Epoch [1360/2000] - Loss: 1.0935\n",
      "Epoch [1365/2000] - Loss: 1.0935\n",
      "Epoch [1370/2000] - Loss: 1.0935\n",
      "Epoch [1375/2000] - Loss: 1.0934\n",
      "Epoch [1380/2000] - Loss: 1.0934\n",
      "Epoch [1385/2000] - Loss: 1.0934\n",
      "Epoch [1390/2000] - Loss: 1.0934\n",
      "Epoch [1395/2000] - Loss: 1.0933\n",
      "Epoch [1400/2000] - Loss: 1.0933\n",
      "Epoch [1405/2000] - Loss: 1.0933\n",
      "Epoch [1410/2000] - Loss: 1.0933\n",
      "Epoch [1415/2000] - Loss: 1.0933\n",
      "Epoch [1420/2000] - Loss: 1.0932\n",
      "Epoch [1425/2000] - Loss: 1.0932\n",
      "Epoch [1430/2000] - Loss: 1.0932\n",
      "Epoch [1435/2000] - Loss: 1.0932\n",
      "Epoch [1440/2000] - Loss: 1.0932\n",
      "Epoch [1445/2000] - Loss: 1.0931\n",
      "Epoch [1450/2000] - Loss: 1.0931\n",
      "Epoch [1455/2000] - Loss: 1.0931\n",
      "Epoch [1460/2000] - Loss: 1.0931\n",
      "Epoch [1465/2000] - Loss: 1.0930\n",
      "Epoch [1470/2000] - Loss: 1.0930\n",
      "Epoch [1475/2000] - Loss: 1.0930\n",
      "Epoch [1480/2000] - Loss: 1.0930\n",
      "Epoch [1485/2000] - Loss: 1.0930\n",
      "Epoch [1490/2000] - Loss: 1.0929\n",
      "Epoch [1495/2000] - Loss: 1.0929\n",
      "Epoch [1500/2000] - Loss: 1.0929\n",
      "Epoch [1505/2000] - Loss: 1.0929\n",
      "Epoch [1510/2000] - Loss: 1.0928\n",
      "Epoch [1515/2000] - Loss: 1.0928\n",
      "Epoch [1520/2000] - Loss: 1.0928\n",
      "Epoch [1525/2000] - Loss: 1.0928\n",
      "Epoch [1530/2000] - Loss: 1.0928\n",
      "Epoch [1535/2000] - Loss: 1.0927\n",
      "Epoch [1540/2000] - Loss: 1.0927\n",
      "Epoch [1545/2000] - Loss: 1.0927\n",
      "Epoch [1550/2000] - Loss: 1.0927\n",
      "Epoch [1555/2000] - Loss: 1.0926\n",
      "Epoch [1560/2000] - Loss: 1.0926\n",
      "Epoch [1565/2000] - Loss: 1.0926\n",
      "Epoch [1570/2000] - Loss: 1.0926\n",
      "Epoch [1575/2000] - Loss: 1.0926\n",
      "Epoch [1580/2000] - Loss: 1.0925\n",
      "Epoch [1585/2000] - Loss: 1.0925\n",
      "Epoch [1590/2000] - Loss: 1.0925\n",
      "Epoch [1595/2000] - Loss: 1.0925\n",
      "Epoch [1600/2000] - Loss: 1.0924\n",
      "Epoch [1605/2000] - Loss: 1.0924\n",
      "Epoch [1610/2000] - Loss: 1.0924\n",
      "Epoch [1615/2000] - Loss: 1.0924\n",
      "Epoch [1620/2000] - Loss: 1.0923\n",
      "Epoch [1625/2000] - Loss: 1.0923\n",
      "Epoch [1630/2000] - Loss: 1.0923\n",
      "Epoch [1635/2000] - Loss: 1.0923\n",
      "Epoch [1640/2000] - Loss: 1.0922\n",
      "Epoch [1645/2000] - Loss: 1.0922\n",
      "Epoch [1650/2000] - Loss: 1.0922\n",
      "Epoch [1655/2000] - Loss: 1.0922\n",
      "Epoch [1660/2000] - Loss: 1.0922\n",
      "Epoch [1665/2000] - Loss: 1.0921\n",
      "Epoch [1670/2000] - Loss: 1.0921\n",
      "Epoch [1675/2000] - Loss: 1.0921\n",
      "Epoch [1680/2000] - Loss: 1.0921\n",
      "Epoch [1685/2000] - Loss: 1.0920\n",
      "Epoch [1690/2000] - Loss: 1.0920\n",
      "Epoch [1695/2000] - Loss: 1.0920\n",
      "Epoch [1700/2000] - Loss: 1.0920\n",
      "Epoch [1705/2000] - Loss: 1.0919\n",
      "Epoch [1710/2000] - Loss: 1.0919\n",
      "Epoch [1715/2000] - Loss: 1.0919\n",
      "Epoch [1720/2000] - Loss: 1.0919\n",
      "Epoch [1725/2000] - Loss: 1.0918\n",
      "Epoch [1730/2000] - Loss: 1.0918\n",
      "Epoch [1735/2000] - Loss: 1.0918\n",
      "Epoch [1740/2000] - Loss: 1.0918\n",
      "Epoch [1745/2000] - Loss: 1.0917\n",
      "Epoch [1750/2000] - Loss: 1.0917\n",
      "Epoch [1755/2000] - Loss: 1.0917\n",
      "Epoch [1760/2000] - Loss: 1.0917\n",
      "Epoch [1765/2000] - Loss: 1.0916\n",
      "Epoch [1770/2000] - Loss: 1.0916\n",
      "Epoch [1775/2000] - Loss: 1.0916\n",
      "Epoch [1780/2000] - Loss: 1.0916\n",
      "Epoch [1785/2000] - Loss: 1.0915\n",
      "Epoch [1790/2000] - Loss: 1.0915\n",
      "Epoch [1795/2000] - Loss: 1.0915\n",
      "Epoch [1800/2000] - Loss: 1.0915\n",
      "Epoch [1805/2000] - Loss: 1.0914\n",
      "Epoch [1810/2000] - Loss: 1.0914\n",
      "Epoch [1815/2000] - Loss: 1.0914\n",
      "Epoch [1820/2000] - Loss: 1.0913\n",
      "Epoch [1825/2000] - Loss: 1.0913\n",
      "Epoch [1830/2000] - Loss: 1.0913\n",
      "Epoch [1835/2000] - Loss: 1.0913\n",
      "Epoch [1840/2000] - Loss: 1.0912\n",
      "Epoch [1845/2000] - Loss: 1.0912\n",
      "Epoch [1850/2000] - Loss: 1.0912\n",
      "Epoch [1855/2000] - Loss: 1.0912\n",
      "Epoch [1860/2000] - Loss: 1.0911\n",
      "Epoch [1865/2000] - Loss: 1.0911\n",
      "Epoch [1870/2000] - Loss: 1.0911\n",
      "Epoch [1875/2000] - Loss: 1.0911\n",
      "Epoch [1880/2000] - Loss: 1.0910\n",
      "Epoch [1885/2000] - Loss: 1.0910\n",
      "Epoch [1890/2000] - Loss: 1.0910\n",
      "Epoch [1895/2000] - Loss: 1.0909\n",
      "Epoch [1900/2000] - Loss: 1.0909\n",
      "Epoch [1905/2000] - Loss: 1.0909\n",
      "Epoch [1910/2000] - Loss: 1.0909\n",
      "Epoch [1915/2000] - Loss: 1.0908\n",
      "Epoch [1920/2000] - Loss: 1.0908\n",
      "Epoch [1925/2000] - Loss: 1.0908\n",
      "Epoch [1930/2000] - Loss: 1.0908\n",
      "Epoch [1935/2000] - Loss: 1.0907\n",
      "Epoch [1940/2000] - Loss: 1.0907\n",
      "Epoch [1945/2000] - Loss: 1.0907\n",
      "Epoch [1950/2000] - Loss: 1.0906\n",
      "Epoch [1955/2000] - Loss: 1.0906\n",
      "Epoch [1960/2000] - Loss: 1.0906\n",
      "Epoch [1965/2000] - Loss: 1.0906\n",
      "Epoch [1970/2000] - Loss: 1.0905\n",
      "Epoch [1975/2000] - Loss: 1.0905\n",
      "Epoch [1980/2000] - Loss: 1.0905\n",
      "Epoch [1985/2000] - Loss: 1.0904\n",
      "Epoch [1990/2000] - Loss: 1.0904\n",
      "Epoch [1995/2000] - Loss: 1.0904\n",
      "Epoch [2000/2000] - Loss: 1.0904\n",
      "Accuracy train: 0.688\n",
      "Accuracy test:  0.684\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# =========================================\n",
    "# 2. Definir el modelo MLP\n",
    "# =========================================\n",
    "class MLP_cls(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        # Capa oculta\n",
    "        self.h1 = nn.Linear(in_features=256, out_features=hidden_size)\n",
    "        # Capa de salida (3 clases)\n",
    "        self.out = nn.Linear(in_features=hidden_size, out_features=3)\n",
    "        # Activación oculta\n",
    "        self.act = nn.ReLU()\n",
    "        # Si quieres, podrías probar Sigmoid, pero ReLU es más usual\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.h1(x)\n",
    "        h1 = self.act(h1)\n",
    "        output = self.out(h1)   # logits (sin softmax)\n",
    "        return output\n",
    "\n",
    "m1 = MLP_cls(hidden_size)\n",
    "print(m1)\n",
    "\n",
    "# Número de parámetros\n",
    "total_params = sum(p.numel() for p in m1.parameters())\n",
    "print('Number of parameters:', total_params)\n",
    "\n",
    "# =========================================\n",
    "# 3. Pasar a tensores de PyTorch\n",
    "# =========================================\n",
    "X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "X_test  = torch.tensor(X_test.to_numpy(),  dtype=torch.float32)\n",
    "\n",
    "# OJO: y debe ser long (int64) y 1D\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "y_test  = torch.tensor(y_test.to_numpy(),  dtype=torch.long)\n",
    "\n",
    "# =========================================\n",
    "# 4. Loss y optimizador\n",
    "# =========================================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(m1.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adam(m1.parameters(), lr=learning_rate)\n",
    "\n",
    "# =========================================\n",
    "# 5. Entrenamiento\n",
    "# =========================================\n",
    "list_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward\n",
    "    outputs = m1(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Guardar loss\n",
    "    list_loss.append(loss.item())\n",
    "\n",
    "    # Backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}] - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# =========================================\n",
    "# 6. Métricas simples en train y test\n",
    "# =========================================\n",
    "with torch.no_grad():\n",
    "    # Train\n",
    "    train_logits = m1(X_train)\n",
    "    train_preds = train_logits.argmax(dim=1)\n",
    "    train_acc = (train_preds == y_train).float().mean().item()\n",
    "\n",
    "    # Test\n",
    "    test_logits = m1(X_test)\n",
    "    test_preds = test_logits.argmax(dim=1)\n",
    "    test_acc = (test_preds == y_test).float().mean().item()\n",
    "\n",
    "print(f\"Accuracy train: {train_acc:.3f}\")\n",
    "print(f\"Accuracy test:  {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "af3269da-9e48-4794-a591-57709710d140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b2d062b350>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d98d2c72-302e-421f-a596-4ca1a6b38b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.6884\n",
      "Training Precision: 0.6884\n",
      "Training Recall   : 0.6884\n",
      "Training F1-score : 0.6883\n",
      "Testing Accuracy : 0.6840\n",
      "Testing Precision: 0.6841\n",
      "Testing Recall   : 0.6840\n",
      "Testing F1-score : 0.6838\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Training metrics\n",
    "# ============================\n",
    "with torch.no_grad():\n",
    "    output_train = m1(X_train)   # forward\n",
    "y_hat_train = output_train.numpy()\n",
    "y_hat_train = [np.argmax(i) for i in y_hat_train]\n",
    "\n",
    "print(f\"Training Accuracy : {accuracy_score(y_train, y_hat_train):.4f}\")\n",
    "print(f\"Training Precision: {precision_score(y_train, y_hat_train, average='macro'):.4f}\")\n",
    "print(f\"Training Recall   : {recall_score(y_train, y_hat_train, average='macro'):.4f}\")\n",
    "print(f\"Training F1-score : {f1_score(y_train, y_hat_train, average='macro'):.4f}\")\n",
    "\n",
    "# ============================\n",
    "# Testing metrics\n",
    "# ============================\n",
    "with torch.no_grad():\n",
    "    output_test = m1(X_test)   # forward\n",
    "y_hat_test = output_test.numpy()\n",
    "y_hat_test = [np.argmax(i) for i in y_hat_test]\n",
    "\n",
    "print(f\"Testing Accuracy : {accuracy_score(y_test, y_hat_test):.4f}\")\n",
    "print(f\"Testing Precision: {precision_score(y_test, y_hat_test, average='macro'):.4f}\")\n",
    "print(f\"Testing Recall   : {recall_score(y_test, y_hat_test, average='macro'):.4f}\")\n",
    "print(f\"Testing F1-score : {f1_score(y_test, y_hat_test, average='macro'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c3d73-cd13-4445-9478-7c7d53a6ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiquetas de clase\n",
    "class_labels = ['BA','ER','WS']\n",
    "\n",
    "# ============================\n",
    "# Matriz de confusión - Train\n",
    "# ============================\n",
    "cm_train = confusion_matrix(y_train, y_hat_train)\n",
    "disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train,\n",
    "                                    display_labels=class_labels)\n",
    "disp_train.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Train\")\n",
    "plt.savefig(\"mlp_train_confusion.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# ============================\n",
    "# Matriz de confusión - Test\n",
    "# ============================\n",
    "cm_test = confusion_matrix(y_test, y_hat_test)\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test,\n",
    "                                   display_labels=class_labels)\n",
    "disp_test.plot(cmap=\"Oranges\", values_format=\"d\")\n",
    "plt.title(\"Test\")\n",
    "plt.savefig(\"mlp_test_confusion.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
