{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9HaGbPvjaHD"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/drive/MyDrive/instances_images\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVE20EILjcT0",
        "outputId": "476b5c5d-5e39-427d-8aef-4d1acb57aa5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Imports (Standard + Array/Image + PyTorch + TorchVision + Sklearn + Viz)\n",
        "# ============================================================\n",
        "\n",
        "# -------- Standard library --------\n",
        "import os\n",
        "import random\n",
        "\n",
        "# -------- Array / image handling --------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# -------- PyTorch --------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "\n",
        "# -------- TorchVision (transforms) --------\n",
        "from torchvision import transforms\n",
        "\n",
        "# -------- Scikit-learn --------\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        ")\n",
        "\n",
        "# -------- Visualization --------\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "yJj0YvxyjcWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/instances_images\"\n"
      ],
      "metadata": {
        "id": "CfjR6g_UjcYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Configuración\n",
        "# ============================================================\n",
        "seed = 42\n",
        "batch_size = 64\n",
        "n_epochs = 29\n",
        "learning_rate = 1e-2\n",
        "train_ratio = 0.8\n",
        "\n",
        "# ============================================================\n",
        "# 1) Semilla\n",
        "# ============================================================\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ============================================================\n",
        "# 2) Transformaciones\n",
        "# ============================================================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# 3) Dataset\n",
        "# ============================================================\n",
        "class GraphImagesDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        valid_exts = (\".png\", \".jpg\", \".jpeg\")\n",
        "        self.image_paths = [\n",
        "            os.path.join(root_dir, f)\n",
        "            for f in os.listdir(root_dir)\n",
        "            if f.lower().endswith(valid_exts)\n",
        "        ]\n",
        "\n",
        "        self.class_names = ['barabasi', 'watts', 'erdos']\n",
        "        self.class_to_idx = {name: idx for idx, name in enumerate(self.class_names)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        filename = os.path.basename(img_path)\n",
        "        prefix = filename.split('_')[0].lower()\n",
        "\n",
        "        if prefix not in self.class_to_idx:\n",
        "            raise ValueError(f\"Clase desconocida en archivo {filename}: {prefix}\")\n",
        "\n",
        "        label = self.class_to_idx[prefix]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "dataset = GraphImagesDataset(data_path, transform=transform)\n",
        "print(\"Total de imágenes:\", len(dataset))\n",
        "\n",
        "# ============================================================\n",
        "# 4) Train + Validación split\n",
        "# ============================================================\n",
        "train_size = int(train_ratio * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_ds, val_ds = random_split(\n",
        "    dataset,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(seed)\n",
        ")\n",
        "\n",
        "print(\"Train:\", len(train_ds), \"Val:\", len(val_ds))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ============================================================\n",
        "# 5) CNN\n",
        "# ============================================================\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Entrada: 3 x 32 x 32\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        # 32x32 -> 16x16 -> 8x8\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 3)  # 3 clases\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)        # 32x32 -> 16x16\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)        # 16x16 -> 8x8\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = Net().to(device)\n",
        "print(model)\n",
        "\n",
        "# ============================================================\n",
        "# 6) Train loop + métricas\n",
        "# ============================================================\n",
        "def train_loop(n_epochs, model, optimizer, loss_fn, train_loader, val_loader, device):\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    metricas_cnn = {\n",
        "        \"epoch\": [],\n",
        "        \"accuracy_train\": [], \"precision_train\": [], \"recall_train\": [], \"f1_train\": [],\n",
        "        \"accuracy_test\":  [], \"precision_test\":  [], \"recall_test\":  [], \"f1_test\":  []  # test = val\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        # -------------------------\n",
        "        # TRAIN\n",
        "        # -------------------------\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        total = 0\n",
        "\n",
        "        y_true_train_all = []\n",
        "        y_pred_train_all = []\n",
        "\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            y_true_train_all.append(labels.detach().cpu().numpy())\n",
        "            y_pred_train_all.append(preds.detach().cpu().numpy())\n",
        "\n",
        "        epoch_train_loss = running_loss / total\n",
        "        train_losses.append(epoch_train_loss)\n",
        "\n",
        "        y_true_train_all = np.concatenate(y_true_train_all)\n",
        "        y_pred_train_all = np.concatenate(y_pred_train_all)\n",
        "\n",
        "        acc_train  = accuracy_score(y_true_train_all, y_pred_train_all)\n",
        "        prec_train = precision_score(y_true_train_all, y_pred_train_all, average=\"macro\", zero_division=0)\n",
        "        rec_train  = recall_score(y_true_train_all, y_pred_train_all, average=\"macro\", zero_division=0)\n",
        "        f1_train   = f1_score(y_true_train_all, y_pred_train_all, average=\"macro\", zero_division=0)\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_total = 0\n",
        "\n",
        "        y_true_val_all = []\n",
        "        y_pred_val_all = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs = imgs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(imgs)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "\n",
        "                val_running_loss += loss.item() * labels.size(0)\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "                preds = outputs.argmax(dim=1)\n",
        "\n",
        "                y_true_val_all.append(labels.detach().cpu().numpy())\n",
        "                y_pred_val_all.append(preds.detach().cpu().numpy())\n",
        "\n",
        "        epoch_val_loss = val_running_loss / val_total\n",
        "        val_losses.append(epoch_val_loss)\n",
        "\n",
        "        y_true_val_all = np.concatenate(y_true_val_all)\n",
        "        y_pred_val_all = np.concatenate(y_pred_val_all)\n",
        "\n",
        "        acc_val  = accuracy_score(y_true_val_all, y_pred_val_all)\n",
        "        prec_val = precision_score(y_true_val_all, y_pred_val_all, average=\"macro\", zero_division=0)\n",
        "        rec_val  = recall_score(y_true_val_all, y_pred_val_all, average=\"macro\", zero_division=0)\n",
        "        f1_val   = f1_score(y_true_val_all, y_pred_val_all, average=\"macro\", zero_division=0)\n",
        "\n",
        "\n",
        "        # Guardar métricas\n",
        "\n",
        "        metricas_cnn[\"epoch\"].append(epoch)\n",
        "\n",
        "        metricas_cnn[\"accuracy_train\"].append(acc_train)\n",
        "        metricas_cnn[\"precision_train\"].append(prec_train)\n",
        "        metricas_cnn[\"recall_train\"].append(rec_train)\n",
        "        metricas_cnn[\"f1_train\"].append(f1_train)\n",
        "\n",
        "        metricas_cnn[\"accuracy_test\"].append(acc_val)\n",
        "        metricas_cnn[\"precision_test\"].append(prec_val)\n",
        "        metricas_cnn[\"recall_test\"].append(rec_val)\n",
        "        metricas_cnn[\"f1_test\"].append(f1_val)\n",
        "\n",
        "        print(\n",
        "            f\"Época {epoch}/{n_epochs} - \"\n",
        "            f\"Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f} | \"\n",
        "            f\"Train Acc: {acc_train:.3f} | Val Acc: {acc_val:.3f} | \"\n",
        "            f\"Train F1: {f1_train:.3f} | Val F1: {f1_val:.3f}\"\n",
        "        )\n",
        "\n",
        "    metricas_cnn_df = pd.DataFrame(metricas_cnn)\n",
        "\n",
        "    metricas_cnn_long = metricas_cnn_df.melt(\n",
        "        id_vars=\"epoch\",\n",
        "        var_name=\"metrica\",\n",
        "        value_name=\"valor\"\n",
        "    )\n",
        "\n",
        "    return train_losses, val_losses, metricas_cnn_df, metricas_cnn_long\n",
        "\n",
        "# ============================================================\n",
        "# 7) Loss + Opt (usando Adam)\n",
        "# ============================================================\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_losses, val_losses, metricas_cnn_df, metricas_cnn_long = train_loop(\n",
        "    n_epochs=n_epochs,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"\\nmetricas_cnn_df (tail):\")\n",
        "print(metricas_cnn_df.tail())\n",
        "\n",
        "print(\"\\nmetricas_cnn_long (head):\")\n",
        "print(metricas_cnn_long.head())\n",
        "\n",
        "# ============================================================\n",
        "# 8) Guardar gráficas Loss y Accuracy (para test y train)\n",
        "# ============================================================\n",
        "plt.figure(figsize=(9, 5))\n",
        "plt.plot(range(1, n_epochs + 1), train_losses, label=\"Train Loss\")\n",
        "plt.plot(range(1, n_epochs + 1), val_losses, label=\"Val Loss\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"loss_cnn.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(9, 5))\n",
        "plt.plot(metricas_cnn_df[\"epoch\"], metricas_cnn_df[\"accuracy_train\"], label=\"Train Accuracy\")\n",
        "plt.plot(metricas_cnn_df[\"epoch\"], metricas_cnn_df[\"accuracy_test\"],  label=\"Val Accuracy\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"accuracy_cnn.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "\n",
        "# (Opcional) Gráfica F1\n",
        "plt.figure(figsize=(9, 5))\n",
        "plt.plot(metricas_cnn_df[\"epoch\"], metricas_cnn_df[\"f1_train\"], label=\"Train F1\")\n",
        "plt.plot(metricas_cnn_df[\"epoch\"], metricas_cnn_df[\"f1_test\"],  label=\"Val F1\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"F1 (macro)\")\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"f1_cnn.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "\n",
        "# ============================================================\n",
        "# 9) Tabla para concatenar con las métricas de los demás modelos\n",
        "# ============================================================\n",
        "# Tu pipeline usa \"fold\". Aquí fold = epoch.\n",
        "metricas_cnn_long = metricas_cnn_long.rename(columns={\"epoch\": \"fold\"})\n",
        "metricas_cnn_long[\"modelo\"] = \"CNN\"\n",
        "\n",
        "print(\"\\nListo: metricas_cnn_long compatible para pd.concat con los otros modelos.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHBG4taufVWr",
        "outputId": "fd11db22-f7bd-41c8-8d3c-7674df67225b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Total de imágenes: 1440\n",
            "Train: 1152 Val: 288\n",
            "Net(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=4096, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=3, bias=True)\n",
            ")\n",
            "Época 1/29 - Train Loss: 1.1609 | Val Loss: 1.0530 | Train Acc: 0.355 | Val Acc: 0.372 | Train F1: 0.315 | Val F1: 0.249\n",
            "Época 2/29 - Train Loss: 0.6489 | Val Loss: 0.3463 | Train Acc: 0.741 | Val Acc: 0.872 | Train F1: 0.733 | Val F1: 0.873\n",
            "Época 3/29 - Train Loss: 0.3444 | Val Loss: 0.2773 | Train Acc: 0.859 | Val Acc: 0.878 | Train F1: 0.858 | Val F1: 0.878\n",
            "Época 4/29 - Train Loss: 0.3079 | Val Loss: 0.2199 | Train Acc: 0.887 | Val Acc: 0.903 | Train F1: 0.887 | Val F1: 0.903\n",
            "Época 5/29 - Train Loss: 0.1488 | Val Loss: 0.1588 | Train Acc: 0.953 | Val Acc: 0.951 | Train F1: 0.953 | Val F1: 0.952\n",
            "Época 6/29 - Train Loss: 0.0979 | Val Loss: 0.0821 | Train Acc: 0.970 | Val Acc: 0.965 | Train F1: 0.970 | Val F1: 0.966\n",
            "Época 7/29 - Train Loss: 0.0490 | Val Loss: 0.0252 | Train Acc: 0.987 | Val Acc: 0.993 | Train F1: 0.987 | Val F1: 0.993\n",
            "Época 8/29 - Train Loss: 0.0240 | Val Loss: 0.0088 | Train Acc: 0.995 | Val Acc: 1.000 | Train F1: 0.995 | Val F1: 1.000\n",
            "Época 9/29 - Train Loss: 0.1146 | Val Loss: 0.0679 | Train Acc: 0.968 | Val Acc: 0.986 | Train F1: 0.968 | Val F1: 0.986\n",
            "Época 10/29 - Train Loss: 0.0575 | Val Loss: 0.0448 | Train Acc: 0.984 | Val Acc: 0.986 | Train F1: 0.983 | Val F1: 0.986\n",
            "Época 11/29 - Train Loss: 0.0302 | Val Loss: 0.0210 | Train Acc: 0.990 | Val Acc: 0.993 | Train F1: 0.990 | Val F1: 0.993\n",
            "Época 12/29 - Train Loss: 0.3425 | Val Loss: 0.5248 | Train Acc: 0.938 | Val Acc: 0.684 | Train F1: 0.938 | Val F1: 0.674\n",
            "Época 13/29 - Train Loss: 0.4490 | Val Loss: 0.2577 | Train Acc: 0.787 | Val Acc: 0.858 | Train F1: 0.787 | Val F1: 0.857\n",
            "Época 14/29 - Train Loss: 0.1159 | Val Loss: 0.0506 | Train Acc: 0.958 | Val Acc: 0.993 | Train F1: 0.958 | Val F1: 0.993\n",
            "Época 15/29 - Train Loss: 0.0574 | Val Loss: 0.0307 | Train Acc: 0.986 | Val Acc: 0.993 | Train F1: 0.986 | Val F1: 0.993\n",
            "Época 16/29 - Train Loss: 0.2880 | Val Loss: 0.1713 | Train Acc: 0.908 | Val Acc: 0.931 | Train F1: 0.908 | Val F1: 0.931\n",
            "Época 17/29 - Train Loss: 0.1176 | Val Loss: 0.0371 | Train Acc: 0.960 | Val Acc: 0.997 | Train F1: 0.960 | Val F1: 0.997\n",
            "Época 18/29 - Train Loss: 0.0204 | Val Loss: 0.0260 | Train Acc: 0.997 | Val Acc: 0.993 | Train F1: 0.997 | Val F1: 0.993\n",
            "Época 19/29 - Train Loss: 0.0159 | Val Loss: 0.0252 | Train Acc: 0.997 | Val Acc: 0.990 | Train F1: 0.997 | Val F1: 0.990\n",
            "Época 20/29 - Train Loss: 0.0101 | Val Loss: 0.0097 | Train Acc: 0.997 | Val Acc: 1.000 | Train F1: 0.997 | Val F1: 1.000\n",
            "Época 21/29 - Train Loss: 0.0469 | Val Loss: 0.0162 | Train Acc: 0.986 | Val Acc: 0.997 | Train F1: 0.986 | Val F1: 0.997\n",
            "Época 22/29 - Train Loss: 0.0578 | Val Loss: 0.0579 | Train Acc: 0.984 | Val Acc: 0.993 | Train F1: 0.984 | Val F1: 0.993\n",
            "Época 23/29 - Train Loss: 0.0419 | Val Loss: 0.0482 | Train Acc: 0.990 | Val Acc: 0.993 | Train F1: 0.990 | Val F1: 0.993\n",
            "Época 24/29 - Train Loss: 0.0309 | Val Loss: 0.0184 | Train Acc: 0.993 | Val Acc: 0.993 | Train F1: 0.993 | Val F1: 0.993\n",
            "Época 25/29 - Train Loss: 0.0114 | Val Loss: 0.0050 | Train Acc: 0.997 | Val Acc: 1.000 | Train F1: 0.997 | Val F1: 1.000\n",
            "Época 26/29 - Train Loss: 0.0087 | Val Loss: 0.0138 | Train Acc: 0.997 | Val Acc: 0.993 | Train F1: 0.997 | Val F1: 0.993\n",
            "Época 27/29 - Train Loss: 0.0059 | Val Loss: 0.0025 | Train Acc: 0.997 | Val Acc: 1.000 | Train F1: 0.997 | Val F1: 1.000\n",
            "Época 28/29 - Train Loss: 0.0078 | Val Loss: 0.0527 | Train Acc: 0.997 | Val Acc: 0.990 | Train F1: 0.997 | Val F1: 0.990\n",
            "Época 29/29 - Train Loss: 0.0098 | Val Loss: 0.0139 | Train Acc: 0.997 | Val Acc: 0.993 | Train F1: 0.997 | Val F1: 0.993\n",
            "\n",
            "metricas_cnn_df (tail):\n",
            "    epoch  accuracy_train  precision_train  recall_train  f1_train  \\\n",
            "24     25        0.996528         0.996519      0.996519  0.996519   \n",
            "25     26        0.997396         0.997391      0.997389  0.997389   \n",
            "26     27        0.997396         0.997391      0.997389  0.997389   \n",
            "27     28        0.997396         0.997391      0.997389  0.997389   \n",
            "28     29        0.997396         0.997391      0.997389  0.997389   \n",
            "\n",
            "    accuracy_test  precision_test  recall_test   f1_test  \n",
            "24       1.000000        1.000000     1.000000  1.000000  \n",
            "25       0.993056        0.993266     0.993127  0.993126  \n",
            "26       1.000000        1.000000     1.000000  1.000000  \n",
            "27       0.989583        0.990000     0.989691  0.989688  \n",
            "28       0.993056        0.993266     0.993127  0.993126  \n",
            "\n",
            "metricas_cnn_long (head):\n",
            "   epoch         metrica     valor\n",
            "0      1  accuracy_train  0.355035\n",
            "1      2  accuracy_train  0.741319\n",
            "2      3  accuracy_train  0.858507\n",
            "3      4  accuracy_train  0.887153\n",
            "4      5  accuracy_train  0.953125\n",
            "\n",
            "Listo: metricas_cnn_long compatible para pd.concat con los otros modelos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metricas_cnn_df.to_excel(\n",
        "    \"metricas_cnn.xlsx\",\n",
        "    index=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "cP_J-hhLhJRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def validate(model, train_loader, val_loader, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "            all_labels = []\n",
        "            all_preds = []\n",
        "\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "                #guardar resultados para métricas\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "            #calcular métricas\n",
        "            acc  = accuracy_score(all_labels, all_preds)\n",
        "            prec = precision_score(all_labels, all_preds, average='macro')\n",
        "            rec  = recall_score(all_labels, all_preds, average='macro')\n",
        "            f1   = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "            print(f\"{name.capitalize()} Accuracy : {acc:.4f}\")\n",
        "            print(f\"{name.capitalize()} Precision: {prec:.4f}\")\n",
        "            print(f\"{name.capitalize()} Recall   : {rec:.4f}\")\n",
        "            print(f\"{name.capitalize()} F1-score : {f1:.4f}\")\n",
        "            print(\"-\"*40)\n",
        "\n",
        "#ejecutar\n",
        "validate(model, train_loader, val_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9TwRNqvi82j",
        "outputId": "1cc969d4-3a92-4b83-b0f2-b9bfbecf1309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy : 0.9991\n",
            "Train Precision: 0.9991\n",
            "Train Recall   : 0.9991\n",
            "Train F1-score : 0.9991\n",
            "----------------------------------------\n",
            "Val Accuracy : 0.9931\n",
            "Val Precision: 0.9933\n",
            "Val Recall   : 0.9931\n",
            "Val F1-score : 0.9931\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}